{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Heart of Helsinki using MS Azure tools"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Import needed libraries"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow.keras\nimport requests\nimport json\nfrom pandas.io.json import json_normalize\nimport functools\nimport asyncio\nimport requests\nimport functools\nfrom ast import literal_eval\nimport time\nfrom datetime import date\n\nimport nest_asyncio\nnest_asyncio.apply()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### API Station request"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "url_station = \"https://api.hypr.cl/station/\"\nheaders_station = {\n    'x-api-key': \"****************************************\",\n    'command': \"list\",\n    'Accept': \"*/*\",\n    'Cache-Control': \"no-cache\",\n    'Host': \"api.hypr.cl\",\n    'Accept-Encoding': \"gzip, deflate\",\n    'Content-Length': \"0\",\n    'Connection': \"keep-alive\",\n    'cache-control': \"no-cache\"\n}\nstation = requests.request(\"POST\", url_station, headers=headers_station)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "station_json = json.loads(station.text)\nstation_df = json_normalize(station_json['list'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "station_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### API Raw request"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "url = \"https://api.hypr.cl/raw/\"\n\nheaders = {\n    'x-api-key': \"iQ0WKQlv3a7VqVSKG6BlE9IQ88bUYQws6UZLRs1B\",\n    'time_start': \"2019-08-01T12:00:00Z\",\n    'time_stop': \"2019-08-01T12:01:00Z\", 'Accept': \"*/*\",\n    'Cache-Control': \"no-cache\",\n    'Host': \"api.hypr.cl\",\n    'Accept-Encoding': \"gzip, deflate\",\n    'Content-Length': \"0\",\n    'Connection': \"keep-alive\",\n    'cache-control': \"no-cache\" }\n\nraw = requests.request(\"POST\", url, headers=headers)\nprint(raw.status_code)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "raw_json = json.loads(raw.text)\nraw_json['raw'][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "raw_df = json_normalize(raw_json['raw'])\nraw_df['time'] = pd.to_datetime(raw_df['time'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "raw_df['hash'].nunique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Define functions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Threaded API raw data loop"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "async def get_hour_data(hour, day, month, year):\n    \n    # Parameters\n    url = \"https://api.hypr.cl/raw/\"\n    batch_size = 10\n    hour_str = \"{:04d}-{:02d}-{:02d}T{:02d}:{:02d}:{}Z\"\n    \n    # Temporary results list for JSON data\n    result = []\n    \n    for i in range(0,int(60/batch_size)):\n        \n        print(\"Loop:\", i+1)\n        \n        loop_start = time.time()\n        \n        for j in range(0, batch_size):\n        \n            datestart = hour_str.format(year, month, day, hour, batch_size*i+j, \"00\")\n            dateend = hour_str.format(year, month, day, hour, batch_size*i+j, \"59\")\n\n            header_str = {\n            'x-api-key': \"iQ0WKQlv3a7VqVSKG6BlE9IQ88bUYQws6UZLRs1B\",\n            'time_start': datestart,\n            'time_stop': dateend,\n            'Accept': \"*/*\",\n            'Cache-Control': \"no-cache\",\n            'Host': \"api.hypr.cl\",\n            'Accept-Encoding': \"gzip, deflate\",\n            'Content-Length': \"0\",\n            'Connection': \"keep-alive\",\n            'cache-control': \"no-cache\"\n            }\n\n            loop = asyncio.get_event_loop()\n            \n            if (j==0):\n                f0 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==1):\n                f1 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==2):\n                f2 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==3):\n                f3 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==4):\n                f4 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==5):\n                f5 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==6):\n                f6 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==7):\n                f7 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==8):\n                f8 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n            elif (j==9):\n                f9 = loop.run_in_executor(None, functools.partial(requests.request, \"POST\", url,\n                                                               headers=header_str));\n        \n        response0 = await f0\n        response1 = await f1\n        response2 = await f2\n        response3 = await f3\n        response4 = await f4\n        response5 = await f5\n        response6 = await f6\n        response7 = await f7\n        response8 = await f8\n        response9 = await f9\n        \n        result.extend(json.loads(response0.text)['raw'])\n        result.extend(json.loads(response1.text)['raw'])\n        result.extend(json.loads(response2.text)['raw'])\n        result.extend(json.loads(response3.text)['raw'])\n        result.extend(json.loads(response4.text)['raw'])\n        result.extend(json.loads(response5.text)['raw'])\n        result.extend(json.loads(response6.text)['raw'])\n        result.extend(json.loads(response7.text)['raw'])\n        result.extend(json.loads(response8.text)['raw'])\n        result.extend(json.loads(response9.text)['raw'])\n        \n        loop_end = time.time()\n        \n        print(\"Loop lasted {} seconds\".format(round(loop_end-loop_start,2)))\n    \n    \n    output_df = json_normalize(result)\n\n    print(\"Time to read JSON to DataFrame:\" , round(time.time() - loop_end,2))\n    #print(\"Total amount of rows:\", output_df.shape[0])\n    \n    return output_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Function for extracting movements from station to another by hash"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def exctract_flow_directions(df):\n    \n    df_grouped = df.groupby(['serial','serial_lead']).nunique()\n    \n    df_result_temp = pd.DataFrame(columns=['serial', 'serial_lead', 'count'])\n    \n    for serial in df['serial'].unique():\n        for serial_lead in df['serial'].unique():\n            \n            if (serial != serial_lead):\n                try:\n                    count = int(df_grouped.loc[(serial, serial_lead), 'hash'])\n\n                    if (df_result_temp[(df_result_temp['serial']==serial_lead) & (df_result_temp['serial_lead']==serial)].shape[0] == 0):\n                        df_result_temp = df_result_temp.append({'serial': serial, 'serial_lead': serial_lead, 'count': count}, ignore_index=True)\n                    else:\n                        df_result_temp = df_result_temp.append({'serial': serial_lead, 'serial_lead': serial, 'count': -count}, ignore_index=True)\n\n                except Exception:\n                    pass\n    \n    df_result_temp_grouped = df_result_temp.groupby(['serial','serial_lead']).sum()\n    \n    df_result = pd.DataFrame(columns=['serial', 'serial_lead', 'count'])\n    \n    for serial in df_result_temp['serial'].unique():\n        for serial_lead in df_result_temp['serial_lead'].unique():\n            try:\n                count = int(df_result_temp_grouped.loc[(serial, serial_lead),'count'])\n\n                if (count > 0):\n                    df_result = df_result.append({'serial': serial, 'serial_lead': serial_lead, 'count': count}, ignore_index=True)\n                else:\n                    df_result = df_result.append({'serial': serial_lead, 'serial_lead': serial, 'count': -count}, ignore_index=True)\n            except Exception:\n                pass\n\n    return df_result",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Run functions in loops"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Count how many unique hashs were at station during each hour"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_df = pd.DataFrame()\n\nday = 14\nmonth = 8\nyear = 2019\nloop_start = time.time()\n\nextracted = pd.DataFrame()\n\nfor hour in range(0,12):\n    \n    print(hour, round(time.time()-loop_start,2))\n    \n    loop = asyncio.get_event_loop()\n    \n    test_df = loop.run_until_complete(get_hour_data(hour, day, month, year))\n    test_df['datetime'] = pd.to_datetime(test_df['time'])\n    test_df['serial_lead'] = (test_df.sort_values(by=['datetime'], ascending=True)\n                       .groupby(['hash'])['serial'].shift(-1)).reset_index()['serial']\n    \n    test_df = exctract_flow_directions(test_df)\n    test_df['hour'] = hour\n    test_df['date'] = date(year=year, month=month, day=day)\n    \n    if (extracted.shape[0] == 0):\n        extracted = test_df\n    else:\n        extracted = extracted.append(test_df)\n\n\nprint(\"Ready! Loop lasted: \", round(time.time()-loop_start,2), \"seconds.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "Count what the people flow streams were between each station during the hour. When flow was to both directions, the amount for one direction is subsctracted from another, and result is positive number representing flow from origin (serial) to destination (serial_lead) station."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from datetime import datetime\nfrom datetime import date\n\ntest_df = pd.DataFrame()\nserial_df_temp = pd.DataFrame()\nserial_df = pd.DataFrame()\n\nday = 14\nmonth = 8\nyear = 2019\nloop_start = time.time()\nfor hour in range(9,12):\n\n    print(hour, round(time.time()-loop_start,2))\n    \n    loop = asyncio.get_event_loop()\n    test_df = loop.run_until_complete(get_hour_data(hour, day, month, year))\n\n\n    test_df['datetime'] = pd.to_datetime(test_df['time'])\n\n    serial_df_temp = test_df.groupby(['serial']).hash.nunique().reset_index(name='nunique')[['serial','nunique']]\n    serial_df_temp['date'] = date(year=year,month=month,day=day)\n    serial_df_temp['hour'] = hour\n\n    if serial_df.shape[0] == 0:\n        serial_df = serial_df_temp\n    else:\n        serial_df = serial_df.append(serial_df_temp)\n\n\nprint(\"Ready! Loop lasted: \", round(time.time()-loop_start,2), \"seconds.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "Lines for saving data to .csv file in Azure Notebooks / Machine Learning Services"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "extracted.to_csv('statdata_{:04d}-{:02d}-{:02d}.csv'.format(year,month,day))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "serial_df.to_csv('flowdata_{:04d}-{:02d}-{:02d}.csv'.format(year,month,day))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "datastore = ws.get_default_datastore()\ndatastore.upload_files(files = ['./flowdata_{:04d}-{:02d}-{:02d}.csv'.format(year,month,day)],\n                       target_path = 'train-dataset/tabular/',\n                       overwrite = True,\n                       show_progress = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check core SDK version number\nimport azureml.core\n\nprint('SDK version:', azureml.core.VERSION)\n\n\nfrom azureml.core import Workspace\n\nws = Workspace.from_config()\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')\n\nexperiment_name = 'train-with-datasets'\n\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)\n\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\ncompute_name = os.environ.get('AML_COMPUTE_CLUSTER_NAME', 'cpu-cluster')\ncompute_min_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MIN_NODES', 0)\ncompute_max_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MAX_NODES', 4)\n\n# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\nvm_size = os.environ.get('AML_COMPUTE_CLUSTER_SKU', 'STANDARD_D2_V2')\n\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes, \n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
